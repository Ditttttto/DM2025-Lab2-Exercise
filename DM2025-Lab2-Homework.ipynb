{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:陳雅郁\n",
    "\n",
    "Student ID:E84111057\n",
    "\n",
    "GitHub ID:Ditttttto\n",
    "\n",
    "Kaggle name:Dittto\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1 Preprocessing Steps**\n",
    "\n",
    "在本次情緒分類的比賽中，我首先針對原始文字資料進行前處理，使其更適合後續的 TF-IDF 以及 BERT 模型輸入。主要的前處理步驟如下：\n",
    "\n",
    "1. **將文字轉為全小寫**  \n",
    "   使用 `text.lower()` 將所有字串統一成小寫，避免同一個字因大小寫不同而被視為不同 token。\n",
    "\n",
    "2. **移除網址（URL）**  \n",
    "   使用正則表示式：`http\\S+|www\\.\\S+`  \n",
    "   將所有以 http / https / www 開頭的網址清除，因為網址並無情緒意義，反而會增加噪音。\n",
    "\n",
    "3. **移除 Twitter @標記（mentions）**  \n",
    "   使用 `@\\w+` 移除像 `@username` 的字串，因為通常不帶情緒語意。\n",
    "\n",
    "4. **移除多餘空白**  \n",
    "   使用 `re.sub(r\"\\s+\", \" \", text).strip()`  \n",
    "   將多個空白壓縮成一個，並移除前後空白，使句子更乾淨。\n",
    "\n",
    "5. **產生 `clean_text` 欄位**  \n",
    "   將清洗後的文字存成新的欄位 `clean_text`，並與原始 `text` 進行前後對照，確認前處理是否正確。\n",
    "\n",
    "6. **情緒標籤數字化（Label Encoding）**  \n",
    "   使用 `LabelEncoder` 將原本的文字標籤（anger, disgust, fear, joy, sadness, surprise）依序轉換成 0–5 的整數標籤。  \n",
    "   此步驟對於模型訓練（尤其是 BERT 的 `num_labels`）相當必要。 \n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Feature Engineering Steps**\n",
    "\n",
    "我採用了 **兩種不同的特徵表示方法**：\n",
    "\n",
    "##### **(1) 傳統 TF-IDF 特徵**\n",
    "\n",
    "我建立了一組 TF-IDF 特徵，用來觀察傳統機器學習表示方式的效果：\n",
    "\n",
    "- `max_features = 20000`：僅保留前 2 萬個重要字詞  \n",
    "- `ngram_range = (1, 2)`：同時使用 unigram 與 bigram  \n",
    "- `stop_words = \"english\"`：移除英文停用字（the, is, and …）\n",
    "\n",
    "TF-IDF 特徵經由：\n",
    "\n",
    "- `fit_transform(X_train)`\n",
    "- `transform(X_val)`\n",
    "\n",
    "得到了稀疏矩陣形式的文字向量，可用於傳統分類器或基線比較。\n",
    "\n",
    "##### **(2) BERT 特徵**\n",
    "\n",
    "資料需轉換成 BERT 能理解的輸入格式：\n",
    "\n",
    "1. **重新切分訓練/驗證資料（80/20）**  \n",
    "   使用 `stratify` 讓六個情緒類別在 train / val 中比例一致。\n",
    "\n",
    "2. **載入 BERT Tokenizer（bert-base-uncased）**  \n",
    "   Tokenizer 會：\n",
    "   - 將句子轉成 `input_ids`  \n",
    "   - 產生 `attention_mask`  \n",
    "   - 對句子做 padding / truncation  \n",
    "\n",
    "3. **自訂 Dataset 類別（EmotionDataset）**  \n",
    "   每筆資料會被編碼成：\n",
    "   - `input_ids`（詞 ID）\n",
    "   - `attention_mask`（用來忽略 padding）\n",
    "   - `labels`（訓練用）\n",
    "\n",
    "4. **統一輸入長度 max_len = 100**  \n",
    "   避免序列過長造成訓練變慢，也避免太短導致語意不足。\n",
    "\n",
    "---\n",
    "\n",
    "### **1.3 Explanation of Your Model**\n",
    "\n",
    "本次主要使用 **HuggingFace Transformers** 中的  **BERT-base-uncased（Bidirectional Encoder Representations from Transformers）**  作為情緒分類模型。\n",
    "\n",
    "##### **（1）模型架構**\n",
    "\n",
    "使用`AutoModelForSequenceClassification(\"bert-base-uncased\", num_labels=6)`，該模型包含：\n",
    "\n",
    "- BERT 12-layer Transformer encoder  \n",
    "- [CLS] token 的 contextual embedding  \n",
    "- 一層線性分類器（classification head）  \n",
    "- Softmax 輸出六個情緒類別的機率  \n",
    "\n",
    "##### **（2）訓練參數（TrainingArguments）**\n",
    "\n",
    "- **Epochs = 3**：BERT 在小型資料集通常訓練 2–4 epoch 就足夠  \n",
    "- **Learning rate = 2e-5**：BERT 常用的 fine-tuning LR  \n",
    "- **Batch size = 16**  \n",
    "- **Weight decay = 0.01**：用於降低 overfitting  \n",
    "- **logging_steps = 500**  \n",
    "\n",
    "\n",
    "##### **（3）Trainer 框架**\n",
    "\n",
    "使用 `Trainer` 自動完成：\n",
    "\n",
    "- dataloader 建置  \n",
    "- forward/backward pass  \n",
    "- 訓練與驗證  \n",
    "- 指標計算（accuracy, precision, recall, f1）  \n",
    "\n",
    "我另外自訂了 `compute_metrics`，使用 weighted f1，以處理情緒類別原本不完全平均的問題。\n",
    "\n",
    "##### **（4）測試集預測與 Kaggle 提交檔**\n",
    "\n",
    "- 使用訓練好的模型對 test 做推論（logits → argmax）  \n",
    "- 用 LabelEncoder 將類別 ID 再轉回情緒名稱（anger, joy…）  \n",
    "- 存成 Kaggle 格式 `submission_bert.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "在本次競賽中，我嘗試了三種完全不同類型的 NLP 方法：\n",
    "\n",
    "- **RNN 的序列模型（BiLSTM）**\n",
    "- **大型生成式模型（Gemini 2.5 Flash）**\n",
    "- **Transformer 微調模型（BERT-base-uncased）**\n",
    "\n",
    "這三種方法的方向、複雜度、訓練方式皆不同。\n",
    "\n",
    "#### **(1) BiLSTM（Bidirectional LSTM）**\n",
    "\n",
    "我首先使用 Keras 建立了一個 BiLSTM 模型，包含：\n",
    "\n",
    "- Tokenizer → integer sequence  \n",
    "- Embedding layer  \n",
    "- BiLSTM（雙向 LSTM）  \n",
    "- Dropout  \n",
    "- Dense + Softmax  \n",
    "\n",
    "提交成果:\n",
    "\n",
    "![BILSTM](pics\\BILSTM.png \"BILSTM\")\n",
    "\n",
    "此結果顯示 BiLSTM 的速度快、參數少，效果中規中矩。\n",
    "\n",
    "#### **(2) Gemini 2.5 Flash**\n",
    "\n",
    "我也嘗試使用了 Google Gemini 2.5 Flash 進行推論。\n",
    "\n",
    "優點：  \n",
    "- 不需要訓練  \n",
    "- 推論速度很快  \n",
    "- 對長句有能力理解語意  \n",
    "\n",
    "缺點：  \n",
    "- 生成式模型不是為分類任務設計  \n",
    "- 預測一致性較差  \n",
    "\n",
    "提交成果:\n",
    "\n",
    "![GEMINI](pics\\GEMINI.png \"GEMINI\")\n",
    "\n",
    "此結果顯示生成式模型，在 Kaggle 這類 supervised benchmark 仍無法與真正 fine-tuned 的模型相比。\n",
    "\n",
    "---\n",
    "\n",
    "#### **(3) BERT-base-uncased（最終採用模型）**\n",
    "\n",
    "我最後使用 HuggingFace Transformers 進行 BERT 微調（fine-tuning），包含：\n",
    "\n",
    "- BERT Tokenizer  \n",
    "- EmotionDataset  \n",
    "- AutoModelForSequenceClassification  \n",
    "- Trainer 訓練三個 epoch  \n",
    "\n",
    "BERT 能夠理解上下文與語意，因此效果最好。\n",
    "提交成果:\n",
    "\n",
    "![BERT](pics\\BERT.png \"BERT\")\n",
    "\n",
    "BERT 能夠理解上下文與語意，在此任務上明顯優於前述兩種方法，是本次競賽最佳模型，因此我選擇用此為最終提交。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "透過這次的實作，我獲得了以下心得：\n",
    "\n",
    "- BiLSTM 能夠快速建立 baseline，但模型能力有限。   \n",
    "- 一直以為Gemini 2.5 Flash 會是最強大的，但結果卻是效果最差的。  \n",
    "- BERT 在情緒分類中表現最穩定且準確，是最佳選擇。   \n",
    "- 大規模預訓練語言模型（PLM）在特定任務中仍需要 fine-tuning 才能發揮最大效果。   \n",
    "- 不同模型在資料量、訓練時間、資源使用上皆有權衡，因此選擇適合任務的模型非常重要。   \n",
    "\n",
    "這些嘗試讓我更清楚理解傳統深度模型、生成式 AI 與 Transformer 微調三者之間的差異與強弱點，對 NLP 模型選擇有更深刻的認識。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 讀取資料\n",
    "train_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Data mining/train_df.csv\")\n",
    "test_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Data mining/test_df.csv\")\n",
    "\n",
    "# 文字清理\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)   # 移除網址\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)              # 移除 @mention\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()      # 縮減空白\n",
    "    return text\n",
    "\n",
    "# 套用清理\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].astype(str).apply(clean_text)\n",
    "test_df[\"clean_text\"]  = test_df[\"text\"].astype(str).apply(clean_text)\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df[\"label\"] = le.fit_transform(train_df[\"emotion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# 載入 BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 訓練/驗證切分\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    train_df[\"clean_text\"],\n",
    "    train_df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"label\"]\n",
    ")\n",
    "\n",
    "# Dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        item = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(X_train_texts, y_train, tokenizer, max_len=100)\n",
    "val_dataset   = EmotionDataset(X_val_texts, y_val, tokenizer, max_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 載入模型\n",
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 評估指標\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# 訓練設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-emotion-results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# Test Prediction\n",
    "X_test_texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "test_dataset = EmotionDataset(X_test_texts, labels=None, tokenizer=tokenizer, max_len=100)\n",
    "\n",
    "pred_output = trainer.predict(test_dataset)\n",
    "logits = pred_output.predictions\n",
    "pred_ids = np.argmax(logits, axis=1)\n",
    "\n",
    "# 轉回原始情緒文字\n",
    "pred_emotions = le.inverse_transform(pred_ids)\n",
    "\n",
    "# 建立 submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"emotion\": pred_emotions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_bert.csv\", index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
